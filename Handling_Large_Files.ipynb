{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNpmp3fVVf4R/EwtCjfDBNS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dmburns1729/Class-Files/blob/main/Handling_Large_Files.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handling Large Files with Low RAM (Optional)"
      ],
      "metadata": {
        "id": "2bEShDsM2kVJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solution\n",
        "Instead of loading the entire dataframe using pd.read_csv, we can instead create a special TextFileReader object, which will allow us to read in our dataframe in chunks.\n",
        "\n",
        "1. Use the chunksize argument for pd.read_csv to create a TextFileReader.\n",
        "    - chunksize is the number of rows to load at once.\n",
        "    - We will use 100,000 rows in our examples.\n",
        "2. Use the .get_chunk() method to extract the first chunk of rows.\n",
        "3. Figure out your entire workflow for that file using just temp_df chunk, and save to disk.\n",
        "4. Now combine the workflow into 1 large loop through the entire textfilereader.\n",
        "5. Use glob to easily combine all chunk csvs into 1 final."
      ],
      "metadata": {
        "id": "Q29MLw2T2qiy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "## Use the chunksize argument for pd.read_csv to create a TextFileReader.\n",
        "basics_url = 'https://datasets.imdbws.com/title.basics.tsv.gz'\n",
        "df_reader = pd.read_csv(basics_url, sep='\\t',\n",
        "                        low_memory=False, chunksize=100_000)\n",
        "df_reader"
      ],
      "metadata": {
        "id": "Lkrxz1qv2nyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "We now get a TextFileReader instead of a DataFrame.\n",
        "The TextFileReader is designed to return one chunk at a time from the source file as a dataframe using the reader.get_chunk() method.\n",
        "It keep tracks of its position in the original file using the ._currow attribute."
      ],
      "metadata": {
        "id": "pgXpVEJV28iE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## the first row # of the next chunk is stored under ._currow\n",
        "df_reader._curro"
      ],
      "metadata": {
        "id": "AuHquFrw3AwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the .get_chunk() method to extract the first chunk of rows.\n",
        "temp_df = df_reader.get_chunk()\n",
        "temp_df"
      ],
      "metadata": {
        "id": "h5iHXq5x3CI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## checking the updated ._currow\n",
        "df_reader._currow"
      ],
      "metadata": {
        "id": "-FOmAhKV3Dhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Replace \"\\N\" with np.nan\n",
        "temp_df.replace({'\\\\N':np.nan},inplace=True)\n",
        "## Eliminate movies that are null for runtimeMinute, genres, and startYear\n",
        "temp_df = temp_df.dropna(subset=['runtimeMinutes','genres','startYear'])"
      ],
      "metadata": {
        "id": "D6uvSuEa3GQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Convert startyear to numeric for slicing\n",
        "temp_df['startYear'] = temp_df['startYear'].astype(float).copy()\n",
        "## keep startYear 2000-2022\n",
        "temp_df = temp_df[(temp_df['startYear']>=2000)&(temp_df['startYear']<2022)]\n",
        "temp_df"
      ],
      "metadata": {
        "id": "jgxJSArS3J8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Make the Data folder if it doesn't already exist\n",
        "import os\n",
        "os.makedirs('Data',exist_ok=True)"
      ],
      "metadata": {
        "id": "ZtIa4Y9F3LN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Programatically saving an fname using the chunk #\n",
        "chunk_num=1\n",
        "fname= f'Data/title_basics_chunk_{chunk_num:03d}.csv.gz'\n",
        "fname"
      ],
      "metadata": {
        "id": "1FkfKr9e6d_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Save temp_df to disk using the fname.\n",
        "temp_df.to_csv(fname, compression='gzip')\n",
        "## incrementing chunk_num by 1 for the next file.\n",
        "chunk_num+=1"
      ],
      "metadata": {
        "id": "3a4f1Dh16fPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.read_csv(fname)"
      ],
      "metadata": {
        "id": "exzI7cte6g7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.read_csv(fname, index_col=0)"
      ],
      "metadata": {
        "id": "mpVK0Jcu6iUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# title basics\n",
        "basics_url = 'https://datasets.imdbws.com/title.basics.tsv.gz'\n",
        "chunk_num = 1\n",
        "df_reader = pd.read_csv(basics_url, sep='\\t',\n",
        "                        low_memory=False, chunksize=100_000)"
      ],
      "metadata": {
        "id": "fOrvsT-56jxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for temp_df in df_reader:\n",
        "        #### COMBINED WORKFLOW FROM ABOVE\n",
        "    ## Replace \"\\N\" with np.nan\n",
        "    temp_df.replace({'\\\\N':np.nan},inplace=True)\n",
        "    ## Eliminate movies that are null for runtimeMinute, genres, and startYear\n",
        "    temp_df = temp_df.dropna(subset=['runtimeMinutes','genres','startYear'])\n",
        "\n",
        "    ## NOTE: THERE ARE ADDITIONAL REQUIRED FILTERING STEPS FOR THE PROJECT NOT SHOWN HERE\n",
        "    ### Convert startyear to numeric for slicing\n",
        "    ## convert numeric features\n",
        "    temp_df['startYear'] = temp_df['startYear'].astype(float)\n",
        "    ## keep startYear 2000-2022\n",
        "    temp_df = temp_df[(temp_df['startYear']>=2000)&(temp_df['startYear']<2022)]\n",
        "\n",
        "    ### Saving chunk to disk\n",
        "    fname= f'Data/title_basics_chunk_{chunk_num:03d}.csv.gz'\n",
        "    temp_df.to_csv(fname, compression='gzip')\n",
        "    print(f\"- Saved {fname}\")\n",
        "\n",
        "    ## increment chunk_num\n",
        "    chunk_num+=1\n",
        "## Closing the reader now that we are done looping through the file\n",
        "df_reader.close()"
      ],
      "metadata": {
        "id": "gMXvfTad6mDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "APPENDIX\n",
        "Bonus functions for getting the size of dataframes and files\n"
      ],
      "metadata": {
        "id": "be2RmuTM6rd6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def get_memory_usage(df,units='mb'):\n",
        "    \"\"\"returns memory size of dataframe in requested units\"\"\"\n",
        "    memory = df.memory_usage().sum()\n",
        "\n",
        "    if units.lower()=='mb':\n",
        "        denom = 1e6\n",
        "    elif units.lower()=='gb':\n",
        "        denom = 1e9\n",
        "    else:\n",
        "        raise Exception('Units must be either \"mb\" or \"gb\"')\n",
        "    val = memory/denom\n",
        "    print(f\"- Total Memory Usage = {val} {units.upper()}\")"
      ],
      "metadata": {
        "id": "-qLI2VcR6nv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_memory_usage(df_combined)\n"
      ],
      "metadata": {
        "id": "gpWQyeu06tIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "copy\n",
        "def get_filesize(fname, units='mb'):\n",
        "    \"\"\"Get size of file at given path in MB or GB\"\"\"\n",
        "    if units.lower()=='mb':\n",
        "        denom = 1e6\n",
        "    elif units.lower()=='gb':\n",
        "        denom = 1e9\n",
        "    else:\n",
        "        raise Exception('Units must be either \"mb\" or \"gb\"')\n",
        "\n",
        "    import os\n",
        "    size = os.path.getsize(fname)\n",
        "\n",
        "    val = size/denom\n",
        "    print(f\"- {fname} is {val} {units.upper()} on disk.\")"
      ],
      "metadata": {
        "id": "fiOnEX7G6uOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_filesize(final_fname)\n"
      ],
      "metadata": {
        "id": "5FENvS5i6vy9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}